{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hopecris31/Reddit_Transformer/blob/master/Copy_of_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reddit Pretrained Transformer\n",
        "\n",
        "```\n",
        "Natural Language Processing\n",
        "CSC-483\n",
        "3/16/2023\n",
        "Authors:\n",
        "Hope Crisafi\n",
        "Claudia Porto\n",
        "Caleb L'Italien\n",
        "Marielise Robillard\n",
        "```"
      ],
      "metadata": {
        "id": "DIk3X60Kr2t_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "149a5SHuTWas"
      },
      "outputs": [],
      "source": [
        "!pip install transformers[sentencepiece]\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install google\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_X5uARsThBh"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "import math\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Model and Tokenizer"
      ],
      "metadata": {
        "id": "D_qIZJtcqLSo"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emsJhlgQUkfV"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "checkpoint = \"distilgpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True, max_length=10)\n",
        "model = AutoModelForCausalLM.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set Google Drive Path (to save model), and load the raw dataset"
      ],
      "metadata": {
        "id": "Tl0pC3JSqXZF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vx330D9lVg8s"
      },
      "outputs": [],
      "source": [
        "eval_dataset_path = \"/content/drive/MyDrive/NLP/eval_dataset\"\n",
        "raw_datasets = load_from_disk(eval_dataset_path)\n",
        "raw_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zFjgVPFsCHv"
      },
      "outputs": [],
      "source": [
        "print(raw_datasets.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define tokenize function to be used in tokenization process"
      ],
      "metadata": {
        "id": "afW8UgrVqg4u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gu0qJZ7PX72w"
      },
      "outputs": [],
      "source": [
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"subreddit\"], example[\"content\"])\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map the tokenized dataset"
      ],
      "metadata": {
        "id": "uTcbdWsoqmqH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rraweS0AYRiR"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True, num_proc=4, remove_columns=raw_datasets.column_names)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LHKf3FZTvpL"
      },
      "outputs": [],
      "source": [
        "tokenized_datasets[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define function to split the tokenized data into block sizes (necessary for training)"
      ],
      "metadata": {
        "id": "5uaOQLWjqpY2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VZ2lhn2724-"
      },
      "outputs": [],
      "source": [
        "block_size = tokenizer.model_max_length\n",
        "\n",
        "def group_texts(examples):\n",
        "    # Concatenate all texts.\n",
        "    concatenated_examples = {k: sum([ex for ex in examples[k] if isinstance(ex, list)], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
        "        # customize this part to your needs.\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    # Split by chunks of max_len.\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Map the tokenized dataset and split into block sizes of size 128"
      ],
      "metadata": {
        "id": "MrP3jEWCqxAm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrfz-nEu8FB8"
      },
      "outputs": [],
      "source": [
        "lm_datasets = tokenized_datasets.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=4,\n",
        ")\n",
        "lm_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test the decoder to make sure that the mapped values can produce text output"
      ],
      "metadata": {
        "id": "GPvnkahMq3td"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wZfsE_I78S18"
      },
      "outputs": [],
      "source": [
        "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the arguments to be used for training"
      ],
      "metadata": {
        "id": "MH7NT9KPq9mA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3MPxkAJBbw9"
      },
      "outputs": [],
      "source": [
        "model_name = checkpoint.split(\"/\")[-1]\n",
        "training_args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-reddit\",\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the trainer using the training_args previously defined, as well as set the parameters for the training methods"
      ],
      "metadata": {
        "id": "blbepNLNrA0f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfKUCRmvBwn5"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=lm_datasets,\n",
        "    eval_dataset=lm_datasets,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model (money line$)"
      ],
      "metadata": {
        "id": "t0d7zMz7rHtU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WGVBSQzqDpH5"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the trained model to Google Drive"
      ],
      "metadata": {
        "id": "3u1o5-nQrTTJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3eo5xgl8LQe"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive')\n",
        "PATH = '/content/drive/MyDrive/NLP' # [CHANGE ME]\n",
        "trainer.save_model (PATH + \"/ChatGRT\")\n",
        "\n",
        "tokenizer.decode(lm_datasets[1][\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the model (prints the perplexity)"
      ],
      "metadata": {
        "id": "95X6ZyxurVzZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RXOLduDcwMdJ"
      },
      "outputs": [],
      "source": [
        "eval_results = trainer.evaluate()\n",
        "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These are the examples to be used to test the BLEU score (refer to report for further explanation)"
      ],
      "metadata": {
        "id": "sKrKnGTiraXR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "input_texts = [\"I had a dog when I was a kid.\",\"Something once happened\",\"The way to do that best is\",\"I am in agreement.\",\"That's perspective is interesting.\",\n",
        "\"That seems overrated to me.\",\"Has anyone else done this?\",\"Thanks, this was helpful.\",\"This is a common misunderstanding.\",\"That is a good argument!\",\n",
        "\"Does anyone have a book suggestion?\",\"I like to do it a different way.\",\"This reminds me of something funny.\",\"I do not get the hype.\",\"Can I get some advice on this?\",\n",
        "\"It depends.\",\"I’m impressed!\",\"I had a similar experience.\",\"I cant believe that’s true.\",\"How should I handle this?\",\"I'm happy I found this community.\",\n",
        "\"This is my favorite sub!\",\"I've always wondered that too.\",\"I think you are right about that.\",\"That is a good question.\",\"I've never heard of that, ever.\",\n",
        "\"I really enjoy hearing about other people’s times.\",\"That’s a great idea!\",\"I have been wanting to do that.\",\"Learning about this is so fascinating.\",\"I'm sorry that happened.\",\n",
        "\"I can relate.\",\"Something similar has happened to me.\",\"Considering this is important.\",\"This resource is extremely valuable.\",\"I'll surely try that!\",\"Thank you for sharing.\",\"I think more people should know about this.\",\n",
        "\"That is a good observation.\",\"I do not agree and this is why.\",\"That idea makes me think.\",\"I'm glad im not alone on this!\",\"This should have more attention.\",\n",
        "\"I've definitely gained a lot from this thread.\",\"This has given me a lot to think about.\",\"I am excited to see what happens next.\",\"I think this discussion is great.\",\"I did know this was a thing.\",\"This is what I was looking for exactly.\",\"I am so grateful to come across this community.\",\n",
        "\"I've always wondered about this.\",\"What do you think about this issue?\",\"I never considered that\",\"This has a lot of potential.\",\"I am curious to see where this goes.\",\n",
        "\"This conversation is important.\",\"I completely agree with this statement.\",\"I appreciate your thoughts about this.\",\"This is a well-stated argument.\",\"Your point is valid.\",\"I had not considered it like that before.\",\n",
        "\"That was a great experience!\",\"I am happy for you!\",\"I like to see different options.\",\"I love that I can learn something new here.\",\"This study really interests me.\",\n",
        "\"I did not know that!\",\"This tip is extremely helpful.\",\"Thanks for telling us.\",\"So glad I came across this post.\"]\n",
        "\n",
        "inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "device = model.device\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "summary_ids = model.generate(**inputs)\n",
        "generated_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "\n",
        "summary_ids"
      ],
      "metadata": {
        "id": "U_91aBAdeAT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_summaries = [\n",
        "[\"I had a childhood dog.\"],[\"Something happened that I\"],[\"The best approach is\"],[\"I totally agree with you.\"],[\"That's an interesting perspective.\"],\n",
        "[\"I think it's overrated, honestly.\"],[\"Did anyone else experience this?\"],[\"I found this helpful, thank you.\"],[\"This is a common misconception.\"],[\"That's a great point!\"],\n",
        "[\"Can anyone recommend a good book?\"],[\"I prefer using a different method.\"],[\"This reminds me of a funny story.\"],[\"I'm not sure I understand the hype.\"],[\"Does anyone have advice on this topic?\"],\n",
        "[\"I think it depends on the situation.\"],[\"Wow, that's really impressive!\"],[\"I had a similar experience once.\"],[\"I can't believe that actually happened.\"],[\"What's the best way to handle this?\"],[\"I'm so glad I found this community.\"],\n",
        "[\"This is my favorite subreddit!\"],[\"I've been wondering the same thing.\"],[\"I think you might be right.\"],[\"That's a really good question.\"],[\"I've never heard of that before.\"],\n",
        "[\"I love hearing about people's experiences.\"],[\"That sounds like a fantastic idea!\"],[\"I've always wanted to try that.\"],[\"It's so fascinating to learn about this.\"],[\"I'm sorry you had to go through that.\"],\n",
        "[\"I can relate to your story.\"],[\"I've been in a similar situation.\"],[\"I think it's important to consider this.\"],[\"This is such a valuable resource.\"],[\"I'll definitely give that a try!\"],[\"Thanks for sharing your thoughts.\"],[\"I wish more people knew about this.\"],\n",
        "[\"That's a really interesting observation.\"],[\"I completely disagree, and here's why.\"],[\"That's a thought-provoking idea.\"],[\"I'm so glad I'm not the only one!\"],[\"This definitely deserves more attention.\"],\n",
        "[\"I've learned so much from this thread.\"],[\"You've given me a lot to think about.\"],[\"I can't wait to see what happens next.\"],[\"I think this is a great discussion.\"],[\"I had no idea this was a thing.\"],[\"This is exactly what I was looking for.\"],[\"I'm really grateful for this community.\"],\n",
        "[\"I've always been curious about this.\"],[\"What are your thoughts on this issue?\"],[\"I never considered that perspective.\"],[\"I think there's a lot of potential here.\"],[\"I'm so excited to see where this goes.\"],\n",
        "[\"This is such an important conversation.\"],[\"I couldn't agree more with this statement.\"],[\"I appreciate your insight on this topic.\"],[\"This is a really well-thought-out argument.\"],[\"That's a very valid point.\"],[\"I hadn't thought about it that way.\"],\n",
        "[\"What an incredible experience!\"],[\"I'm so happy for you!\"],[\"It's great to see different opinions.\"],[\"I always learn something new here.\"],[\"That's a really interesting study.\"],\n",
        "[\"I never knew that!\"],[\"This is such a helpful tip.\"],[\"Thanks for sharing your experience.\"],[\"I'm so glad I found this post.\"]\n",
        "]"
      ],
      "metadata": {
        "id": "HJdhrluGfIKt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the BLEU score (again, refer to report)"
      ],
      "metadata": {
        "id": "ChZ4mQBWroyX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_gT760S2r1iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary_ids = model.generate(**inputs)\n",
        "generated_summaries = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
        "\n",
        "tokenized_generated_summaries = [summary.split() for summary in generated_summaries]\n",
        "bleu_score = corpus_bleu(reference_summaries, tokenized_generated_summaries, smoothing_function=smooth_fn)\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score:.2f}\")"
      ],
      "metadata": {
        "id": "KSNjoiXgqskx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = lm_datasets[1][\"input_ids\"]\n",
        "reference_summaries = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "bleu_score = corpus_bleu([[ref] for ref in reference_summaries], generated_summaries)\n",
        "\n",
        "print(f\"BLEU Score: {bleu_score:.2f}\")"
      ],
      "metadata": {
        "id": "EZnp_In-dIia"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}